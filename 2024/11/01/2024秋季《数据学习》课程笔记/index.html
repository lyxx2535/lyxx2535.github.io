<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Annie">





<title>2024秋季《数据学习》课程笔记 | Annie&#39;s Blog</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Annie&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Annie&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; 菜单</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">全部展开</a>
        <a onclick="go_top()">回到顶部</a>
        <a onclick="go_bottom()">前往底部</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">2024秋季《数据学习》课程笔记</h1>
            
                <div class="post-meta">
                    
                        作者: <a itemprop="author" rel="author" href="/">Annie</a>
                    

                    
                        <span class="post-time">
                        日期: <a href="#">November 1, 2024&nbsp;&nbsp;21:54:05</a>
                        </span>
                    
                    
                        <span class="post-category">
                        分类:
                            
                                <a href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/">研究生课程</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="lec1-介绍"><a href="#lec1-介绍" class="headerlink" title="lec1 介绍"></a>lec1 介绍</h1><h2 id="机器学习任务"><a href="#机器学习任务" class="headerlink" title="机器学习任务"></a>机器学习任务</h2><ul>
<li>分类（classification）: 将实例数据划分到合适的类别中。例如 判断网站是否被黑客入侵（二分类 ），手写数字的自动识别（多分类），多目标分类（多分类）。</li>
<li>回归（regression）: 主要用于预测数值型数据。例如: 股票价格波动的预测，房屋价格的预测等。建模和分析变量之间的关系。找函数，将输入转为数值输出就是回归。</li>
<li>识别：如把语音识别成文字</li>
<li>图片降噪</li>
<li>anomaly detection异常检测：如发现网络活动中的异常操作</li>
<li>generation：生成</li>
<li>decision making：机器人做什么反应和动作</li>
</ul>
<h2 id="性能评估"><a href="#性能评估" class="headerlink" title="性能评估"></a>性能评估</h2><p>这里MAE其实就是|y-f(x)|实际值与预测值差的绝对值，只有不相等时为1</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410292225058.png" alt="image-20241029222508023"></p>
<h2 id="学习的种类"><a href="#学习的种类" class="headerlink" title="学习的种类"></a>学习的种类</h2><ol>
<li>有监督学习：给输入x，经过f，输出y <ol>
<li>分类：y是离散的</li>
<li>回归：y是连续的（预测股市收盘价，image2text，video2text）</li>
</ol>
</li>
<li>无监督学习：没有先验标签，自己找数据中潜在的模式和结构<ol>
<li>数据聚类：将数据集中的样本划分为若干个不重叠的子集或“簇”，使得同一个簇内的样本在某种意义上更相似，而不同簇之间的样本则相对较不相似。可以帮助理解数据的分布和结构，为进一步的分析和决策提供支持。</li>
<li>异常检测：识别数据集中不符合预期模式或与其他数据显著不同的数据点（异常值或离群点）的方法。关键挑战是如何平衡误报率和漏报率。</li>
</ol>
</li>
<li>强化学习：动态环境中学习，agent做连续决策（根据反馈的reward）</li>
</ol>
<h2 id="Inference推理-VS-Prediction预测"><a href="#Inference推理-VS-Prediction预测" class="headerlink" title="Inference推理 VS Prediction预测"></a>Inference推理 VS Prediction预测</h2><ul>
<li>前者知道f的结构，找好的模型描述f（关注统计，数据分布，寻找统计模型的参数）</li>
<li>后者给未来的数据，预测使用f的输出y（关注ML）</li>
</ul>
<h2 id="ML历史"><a href="#ML历史" class="headerlink" title="ML历史"></a>ML历史</h2><blockquote>
<p>年份肯定不会考，其他模型的特点啥的可以看看</p>
</blockquote>
<p>在高维空间中，可能的配置x的数量远大于训练样本的数量。（数据点可能的排列组合方式非常多，而实际可用的训练样本数量相比之下则显得非常少。）</p>
<ul>
<li>半监督学习：从少量标记数据和大量未标记数据中学习</li>
<li>主动学习：一种半监督学习类型，通过与用户交互式查询来获取新数据点的标签。</li>
<li>自监督学习：利用输入数据中的固有结构或关系来创建有意义的特征。标签是从数据本身学来的</li>
</ul>
<p>异构学习：涉及不同数据类型、设备、模型的异构性</p>
<h1 id="Lec2-1-线性回归"><a href="#Lec2-1-线性回归" class="headerlink" title="Lec2.1 线性回归"></a>Lec2.1 线性回归</h1><blockquote>
<p>可以推导到非线性最小二乘</p>
<p>超平面和wt向量是垂直的</p>
</blockquote>
<p>复习有监督学习：找f建模X-&gt;Y</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>两种写法：一种是用θ覆盖w&#x2F;b，一种是w&#x2F;b单列</p>
<p>都是线性</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301304161.png" alt="image-20241030130417094"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301304844.png" alt="image-20241030130443817"></p>
<h2 id="两种参数预测方式"><a href="#两种参数预测方式" class="headerlink" title="两种参数预测方式"></a>两种参数预测方式</h2><h3 id="最小二乘LSE"><a href="#最小二乘LSE" class="headerlink" title="最小二乘LSE"></a>最小二乘LSE</h3><p>几何方法</p>
<p>需要最小化损失函数：<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301310288.png" alt="image-20241030131016261" style="zoom:50%;" /></p>
<h4 id="数值解"><a href="#数值解" class="headerlink" title="数值解"></a>数值解</h4><h5 id="1-梯度下降"><a href="#1-梯度下降" class="headerlink" title="1. 梯度下降"></a>1. 梯度下降</h5><p>分为BGD和SGD</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301313121.png" alt="image-20241030131351075"></p>
<p>因为发现J(θ)是凸函数（二阶导&gt;&#x3D;0），所以可以用梯度下降</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301318301.png" alt="image-20241030131821260"></p>
<h6 id="BGD批量梯度下降"><a href="#BGD批量梯度下降" class="headerlink" title="BGD批量梯度下降"></a>BGD批量梯度下降</h6><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301323821.png" alt="image-20241030132301790"></p>
<p>θ在使用m个训练样本后才会被更新</p>
<h6 id="SGD随机梯度下降"><a href="#SGD随机梯度下降" class="headerlink" title="SGD随机梯度下降"></a>SGD随机梯度下降</h6><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301323059.png" alt="image-20241030132359029"></p>
<p>每次读取一个样本都会更新一次</p>
<ol>
<li>随机梯度下降使θ接近最小更快</li>
<li>有利于大数据回归</li>
</ol>
<h5 id="2-牛顿法"><a href="#2-牛顿法" class="headerlink" title="2. 牛顿法"></a>2. 牛顿法</h5><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301417527.png" alt="image-20241030141710430"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301415543.png" alt="image-20241030141538523"></p>
<p>牛顿方法的性能：</p>
<ul>
<li>相比批量梯度下降需要更少的迭代次数</li>
<li>计算H^−1是耗时的</li>
<li>n很小时在实践中更快</li>
</ul>
<h4 id="分析解：-正规方程"><a href="#分析解：-正规方程" class="headerlink" title="分析解： 正规方程"></a>分析解： 正规方程</h4><p>利用设计矩阵X和向量y（大写为矩阵，小写为向量，下图X写的不标准）</p>
<p>属于闭合解（与数值解相对，是公式解得的）</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301352653.png" alt="image-20241030135254548"></p>
<p>X^TX可逆的条件是矩阵满秩，即行列式为0</p>
<h4 id="梯度下降VS正规方程"><a href="#梯度下降VS正规方程" class="headerlink" title="梯度下降VS正规方程"></a>梯度下降VS正规方程</h4><table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody><tr>
<td>迭代解</td>
<td>精确解</td>
</tr>
<tr>
<td>代价函数需要选择合适的学习参数α来收敛</td>
<td>当X是病态时，数值不稳定。例如，特征是高度相关的，X^TX行列式接近0</td>
</tr>
<tr>
<td>适用于大量样本m</td>
<td>当m较大时，求解方程的速度较慢</td>
</tr>
</tbody></table>
<p>现在DL的数据集很大，所以用GD而不是分析解</p>
<h3 id="极大似然MLE"><a href="#极大似然MLE" class="headerlink" title="极大似然MLE"></a>极大似然MLE</h3><p>概率方法，认为ε是噪声，人为建模ε是对高斯分布N(0,σ^2)的独立同分布。在该对ε的假设下，θ的最小二乘对应着极大似然。</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301457193.png" alt="image-20241030145714137"></p>
<h1 id="Lec2-2-逻辑回归"><a href="#Lec2-2-逻辑回归" class="headerlink" title="Lec2.2 逻辑回归"></a>Lec2.2 逻辑回归</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>传统形式也是用于二分类，相比线性回归增加了sigmoid函数，进行归一化（如把像素映射到[0,1]）</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301503464.png" alt="image-20241030150343440"></p>
<h2 id="参数预测"><a href="#参数预测" class="headerlink" title="参数预测"></a>参数预测</h2><p>假设y|x符合伯努利分布：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301513664.png" alt="image-20241030151326630"></p>
<p>希望l(θ)最大，因为是凹函数取导数&#x3D;0，使用梯度上升：<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301547530.png" alt="image-20241030154747496"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301547587.png" alt="image-20241030154740492"></p>
<h1 id="Lec2-3-多分类"><a href="#Lec2-3-多分类" class="headerlink" title="Lec2.3 多分类"></a>Lec2.3 多分类</h1><h2 id="One-vs-Rest：从类别入手"><a href="#One-vs-Rest：从类别入手" class="headerlink" title="One vs Rest：从类别入手"></a>One vs Rest：从类别入手</h2><p>k个类别，学习k种模型，变成k个二分类问题，选概率最大的即可</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301612633.png" alt="image-20241030161232593"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301612146.png" alt="image-20241030161254111"></p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol>
<li>类不平衡：负样本比正样本多</li>
<li>不同的分类器可能有不同的置信度（决策边界不同）</li>
</ol>
<p>为了改善这个缺点，我们希望为所有类学一个模型——Softmax</p>
<h2 id="Softmax：从算法入手"><a href="#Softmax：从算法入手" class="headerlink" title="Softmax：从算法入手"></a>Softmax：从算法入手</h2><p>多项式分布用于模拟一个k面的骰子（或任何有k个可能结果的实验）被掷m次的结果。</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301632242.png" alt="image-20241030163211208"></p>
<h3 id="参数预测-1"><a href="#参数预测-1" class="headerlink" title="参数预测"></a>参数预测</h3><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301635710.png" alt="image-20241030163558676"></p>
<p>求导过程看起来很复杂，老师上课没讲，就这里带过：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301647856.png" alt="image-20241030164725826"></p>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ol>
<li>标量加法的不变性：<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301649409.png" alt="image-20241030164939389" style="zoom:50%;" /></li>
<li>k&#x3D;2时为逻辑回归，可写作sigmoid：<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301653992.png" alt="image-20241030165342955"></li>
</ol>
<h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><p>当类是互斥的：使用Softmax</p>
<p>不互斥：多个二进制分类器可能更好</p>
<h1 id="Lec2-4-广义线性模型GLM"><a href="#Lec2-4-广义线性模型GLM" class="headerlink" title="Lec2.4 广义线性模型GLM"></a>Lec2.4 广义线性模型GLM</h1><h2 id="标准形式"><a href="#标准形式" class="headerlink" title="标准形式"></a>标准形式</h2><p>y|x; θ来自一个指数族</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301703363.png" alt="image-20241030170358333"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301712216.png" alt="image-20241030171213169"></p>
<p>T,a,b确定一种分布，η是该分布的参数</p>
<p><em>a</em>(<em>η</em>) 是对数划分函数，确保概率归一化，所以有以下和为1的性质：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301717422.png" alt="image-20241030171713378"></p>
<p>优点：是线性回归模型的扩展，可以非线性</p>
<ol>
<li>广义线性模型允许目标变量服从<strong>指数族分布</strong>，而不仅仅是正态分布。这使得广义线性模型能够处理二元数据（例如逻辑回归）、计数数据（例如泊松回归）</li>
<li>通过引入连接函数，广义线性模型可以捕捉变量之间的<strong>非线性关系</strong>。例如，逻辑回归使用对数几率连接函数来处理二元分类问题中的非线性关系。</li>
</ol>
<h3 id="化解步骤"><a href="#化解步骤" class="headerlink" title="化解步骤"></a>化解步骤</h3><p>如何化成标准形式？</p>
<ol>
<li>如果式子没有exp可以取对数再还原成exp</li>
<li>有exp后，分离出含有y的部分（合并同类项）</li>
</ol>
<p>例1：高斯分布</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301932001.png" alt="image-20241030193210954"></p>
<p>例2：伯努利分布</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410301931676.png" alt="image-20241030193149617"></p>
<p>例3：多分类</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302059795.png" alt="img"></p>
<p><img src="https://pic1.zhimg.com/v2-56e1db0295fbc17fe72c1526e9f27510_b.png" alt="img"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302112836.png" alt="img"></p>
<p><img src="https://picx.zhimg.com/v2-35f524d590d0107aee2335ba549e7803_b.png" alt="img"></p>
<p><img src="https://pic3.zhimg.com/v2-5c20bc10f029f30e5f3183b8719ac742_b.png" alt="img"></p>
<p><img src="https://picx.zhimg.com/v2-d596b3f56a582983a57b44c69a5d6cd3_b.png" alt="img"></p>
<h2 id="建模步骤"><a href="#建模步骤" class="headerlink" title="建模步骤"></a>建模步骤</h2><ol>
<li>定义y|x;θ属于的指数分布族</li>
<li>求解假设函数h(x)&#x3D;E[T(y)|x]。其中T(y)大部分时候为y</li>
<li>自然参数η和x是线性关系，η&#x3D;θ^T x</li>
<li>response function：g(η)&#x3D;E[T(y)|x]</li>
<li>link function：η&#x3D;g^-1(E[T(y)|x])</li>
</ol>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302054647.png" alt="image-20241030205412589"></p>
<p>例1：线性回归</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302101204.png" alt="image-20241030210148139"></p>
<p>例2：伯努利分布</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302104541.png" alt="image-20241030210417478"></p>
<p>例3：多分类</p>
<p><img src="https://pic1.zhimg.com/v2-eb28bdcea76b56ba707376cd7a7ef1f6_b.png" alt="img"></p>
<p>response和link：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302113872.png" alt="image-20241030211354821"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>y是种类</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302114143.png" alt="image-20241030211442101"></p>
<h1 id="Lec3-生成式学习算法"><a href="#Lec3-生成式学习算法" class="headerlink" title="Lec3 生成式学习算法"></a>Lec3 生成式学习算法</h1><p>GDA和NB是两个生成学习的例子</p>
<h2 id="判别式vs生成式"><a href="#判别式vs生成式" class="headerlink" title="判别式vs生成式"></a>判别式vs生成式</h2><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302125249.png" alt="image-20241030212517190"></p>
<ul>
<li>discriminative判别式：学习p(y|x)，区分数据点的类别<ul>
<li>如线性回归，逻辑回归，k-最近邻，神经网络，SVM，决策树，softmax（多分类）</li>
<li>GLM建模的是p(y|x;θ)，所以是判别式模型</li>
<li>极大似然估计的对象L(θ)&#x3D;p(y|x)的连乘</li>
</ul>
</li>
<li>generative生成式：学习p(x,y)<ul>
<li>其实就是学习p(x|y)和p(y)，然后用贝叶斯</li>
<li>p(y)是类先验</li>
<li>极大似然估计的对象L(θ)&#x3D;p(x,y)的连乘</li>
<li>如高斯判别分析（连续输入下），朴素贝叶斯（离散输入下），隐马尔可夫模型</li>
</ul>
</li>
</ul>
<h2 id="高斯判别分析GDA-LDA"><a href="#高斯判别分析GDA-LDA" class="headerlink" title="高斯判别分析GDA&#x2F;LDA"></a>高斯判别分析GDA&#x2F;LDA</h2><ol>
<li>选择一个数据生成分布：假设p(x|y)和p(y)</li>
<li>估计4个参数</li>
<li>计算p(y|x)来预测新输入样本x’的输出label</li>
</ol>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302129454.png" alt="image-20241030212922415"></p>
<h3 id="前置知识：多元正态分布"><a href="#前置知识：多元正态分布" class="headerlink" title="前置知识：多元正态分布"></a>前置知识：多元正态分布</h3><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302131743.png" alt="image-20241030213142683"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302131091.png" alt="image-20241030213154061"></p>
<p>Σ控制着分布的扩散：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302135495.png" alt="image-20241030213503434"></p>
<p>当Σ的非对角线项非零时，分布不再沿轴定向：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302135185.png" alt="image-20241030213535131"></p>
<h3 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h3><ol>
<li><p>建模P(y)，P(x|y)<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302137539.png" alt="image-20241030213706479"></p>
</li>
<li><p>使用极大似然估计求4个参数的值<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302137872.png" alt="image-20241030213737820"></p>
</li>
<li><p>计算p(y|x)来预测label，发现此时的p(y|x)是逻辑函数<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302141354.png" alt="image-20241030214109304"></p>
</li>
</ol>
<p>通过极大似然得到的后验概率有p(y&#x3D;1|x)&#x3D;p(y&#x3D;0|x)&#x3D;0.5，即θ^Tx&#x3D;0，它最大化了类间分离【是线性模型】</p>
<h3 id="与判别式逻辑回归对比"><a href="#与判别式逻辑回归对比" class="headerlink" title="与判别式逻辑回归对比"></a>与判别式逻辑回归对比</h3><p>GDA：</p>
<ul>
<li>最大化p(x,y)</li>
<li>建模p(x|y),p(y)</li>
<li>当模型假设正确时，GDA是渐近有效和数据有效的。这意味着随着样本量的增加，GDA的参数估计将收敛到真实的参数值，并且它可以使用较少的数据达到相同的估计精度。</li>
</ul>
<p>LR：</p>
<ul>
<li>最大化p(y|x)</li>
<li>建模p(y|x)。和GDA不同，对p(x)没有限制。</li>
<li>对不正确的建模假设更健壮和更不敏感</li>
</ul>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302205712.png" alt="image-20241030220509664"></p>
<h2 id="朴素贝叶斯NB"><a href="#朴素贝叶斯NB" class="headerlink" title="朴素贝叶斯NB"></a>朴素贝叶斯NB</h2><p>一个简单的离散输入变量生成学习算法例子：</p>
<ul>
<li>垃圾邮件过滤器（文档分类）将电子邮件消息x分类为垃圾邮件（y &#x3D; 1）和非垃圾邮件（y &#x3D; 0）类。</li>
<li>二进制文本特征：给定一个大小为n的字典，表示一个由字典单词组成的消息，如x∈{0,1}^n。如果某一位为1，所以字典中的第i个单词在这个消息中。</li>
</ul>
<h3 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h3><p>假设在给定类别y的情况下，各个特征xi是条件独立的（朴素就是指的条件独立）。<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302308325.png" alt="image-20241030230806277"></p>
<p>多元伯努利事件模型x|y是由个独立的伯努利试验生成的。模型的参数是人为假设的，让y和x|y满足伯努利分布。【注意】伯努利只是一种NB，适用于二元随机变量分布，这里举例来求参数，并不是一定符合。生成式学习算法的逻辑依然符合：</p>
<ol>
<li><p>建模P(y)，P(x|y)<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302314418.png" alt="image-20241030231426370"></p>
</li>
<li><p>计算参数（极大似然p(x,y)）<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302327328.png"></p>
</li>
<li><p>计算p(y|x)来预测label，T是判断类的阈值</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410302333099.png" alt="image-20241030233314051"></p>
</li>
</ol>
<h2 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h2><p>朴素贝叶斯中，如果某个单词x_j在训练数据中从未出现过（没观察到），那么概率都为0，会有分母为0，所以在分子分母做处理</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311508637.png" alt="image-20241031150802542"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311536317.png" alt="image-20241031153608270"></p>
<p>上面介绍了<strong>Multi-variate Bernoulli model</strong>，下面是<strong>Multinomial event model</strong>，区别是上面的x|y服从伯努利，下面的服从多项式</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311554859.png" alt="image-20241031155403806"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311554093.png" alt="image-20241031155410045"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311554602.png" alt="image-20241031155417565"></p>
<h1 id="Lec4-SVM"><a href="#Lec4-SVM" class="headerlink" title="Lec4 SVM"></a>Lec4 SVM</h1><p>SVM是判别式模型</p>
<p>线性模型：LR，岭回归，逻辑回归，GDA</p>
<p>非线性模型：多项式回归softmax，SVM，神经网络</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311619350.png" alt="image-20241031161924312"></p>
<p>前面方法的问题：</p>
<ul>
<li>模型的选择影响模型性能；容易导致模型不匹配。</li>
<li>数据往往采样不均匀，在高维输入空间中形成稀疏分布。这导致了一些感觉不舒服的问题</li>
</ul>
<p>可能的方法：</p>
<ul>
<li>正则化</li>
<li>稀疏核方法</li>
</ul>
<h2 id="Linear-SVM"><a href="#Linear-SVM" class="headerlink" title="Linear SVM"></a>Linear SVM</h2><p>Margin边际：决策边界与任何样本之间的最小距离（边际也表示分类置信度）</p>
<p>如何确定边际？</p>
<h3 id="functionally"><a href="#functionally" class="headerlink" title="functionally"></a>functionally</h3><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311629092.png" alt="image-20241031162906046"></p>
<p>当前方式的问题是与w&#x2F;b有关，如果 w′&#x3D;2w和 b′&#x3D;2b，那么决策边界（即分类边界）保持不变，但是边际表达式会改变，所以使用geometric方法进行归一化</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311645785.png" alt="image-20241031164555726"></p>
<h3 id="geometrically"><a href="#geometrically" class="headerlink" title="geometrically"></a>geometrically</h3><p>括号内的正好是样本点到超平面的距离</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311658282.png" alt="image-20241031165827226"></p>
<p>目标是最大化γ。令函数边际γ_hat固定为1，最终化为min</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311701840.png" alt="image-20241031170141780"></p>
<h2 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h2><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311709834.png" alt="image-20241031170912777"></p>
<p>拉格朗日对偶性（Lagrange Duality）：指出f和所有gi都是凸的，hi都是仿射的（x-&gt;Ax+b），并存在某个 w 使得 gi(w)&lt;0对所有 i 成立（严格可行），那么存在 w∗,α∗,β使得 w∗ 是原始问题的解，而 α∗,β 是对偶问题的解，并且：<em>p</em>∗&#x3D;<em>d</em>∗&#x3D;<em>L</em>(<em>w</em>∗,<em>α</em>∗,<em>β</em>∗)</p>
<ul>
<li>注意max min&lt;&#x3D;min max</li>
</ul>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311712882.png" alt="image-20241031171257812"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311711929.png" alt="image-20241031171102867"></p>
<h2 id="参数预测-2"><a href="#参数预测-2" class="headerlink" title="参数预测"></a>参数预测</h2><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311801698.png" alt="image-20241031180140584"></p>
<h2 id="Soft-Margin-SVM"><a href="#Soft-Margin-SVM" class="headerlink" title="Soft Margin SVM"></a>Soft Margin SVM</h2><p>linear svm无法处理极端值和非线性情况，而soft引入松弛向量，允许一些数据点分类错误。C代表误分类的惩罚程度</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311957164.png" alt="image-20241031195735094"></p>
<p>当使用线性核，soft仍然是线性；使用kernel trick可以扩展到非线性</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410311958854.png" alt="image-20241031195832795"></p>
<p>图里的λ就是r</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410222223073.png" alt="image-20241022222347998"></p>
<p>依然是先算α再算w&#x2F;b</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312000861.png" alt="image-20241031200005817"></p>
<p>就像线性中满足wx+b&#x3D;1的是sv，这里满足1-ε&#x3D;wx+b的是sv（边界上+违反约束在边界内的错误分类点）TODO：还是不懂下面的λ和ξ的推导逻辑</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410222220339.png" alt="image-20241022222052235"></p>
<h2 id="kernel-svm非线性"><a href="#kernel-svm非线性" class="headerlink" title="kernel svm非线性"></a>kernel svm非线性</h2><p>将不可分离的数据映射到高维就可分离了</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312023562.png" alt="image-20241031202304481"></p>
<p>在优化参数时不用算w：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312024992.png" alt="image-20241031202427924"></p>
<p>不是所有函数都能当核函数，要满足mercer定理</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312027297.png" alt="image-20241031202737207"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312031949.png" alt="image-20241031203147900"></p>
<p>计算过程：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312032727.png" alt="image-20241031203242672"></p>
<h1 id="Lec5-DNN"><a href="#Lec5-DNN" class="headerlink" title="Lec5 DNN"></a>Lec5 DNN</h1><p>阈值函数&#x2F;激活函数</p>
<p>单个神经元是一个（线性）二值分类器：</p>
<ul>
<li>当f是sigmoid函数，等价于二值softmax(1&#x2F;1+e^-x)</li>
<li>当f是sign函数，等价于感知器</li>
</ul>
<p>DNN的目标是建模f(x)，定义测量f的预测误差的误差函数：例如，在分类中使用的一个常见的误差函数是对数损失a.k.a.交叉熵损失：<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312147840.png" alt="image-20241031214716810"></p>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312148662.png" alt="image-20241031214858578"></p>
<p>至少有一层hidden layer。不同的w有不同的线性模型</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312157562.png" alt="image-20241031215701509"></p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>a是hidden层激活的结果变成当前节点，赋予w权重，以及加上b，得到的中间变量；h(x)是对a赋予激活函数的结果。最后的输出是最后的h<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312210818.png" alt="image-20241031221017724"></p>
<p>参数选择就是想最小化损失函数</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312246657.png" alt="image-20241031224648585"></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40378224">反向传播</a>就是使输出值和目标值的差距尽可能小，反复使用链式法则，返回梯度下降</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010002627.png" alt="image-20241101000233543"></p>
<p>ppt中把dL&#x2F;dy认为是1，再进行求解，其实是一种简略的写法</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010041823.png" alt="image-20241101004153730"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010058475.png" alt="image-20241101005816380"></p>
<h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p>相比增加width，增加神经网络的depth更有效（增大容量）会过拟合</p>
<p>用正则化避免：<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410312204642.png" alt="image-20241031220444553"></p>
<h1 id="Lec6-模型选择"><a href="#Lec6-模型选择" class="headerlink" title="Lec6 模型选择"></a>Lec6 模型选择</h1><h2 id="误差分类"><a href="#误差分类" class="headerlink" title="误差分类"></a>误差分类</h2><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010118364.png" alt="image-20241101011813312">泛化&#x2F;测试误差：不在训练集上测</p>
<p>分类：y是离散的；回归：y是连续的</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010119842.png" alt="image-20241101011926693"></p>
<h2 id="性质-1"><a href="#性质-1" class="headerlink" title="性质"></a>性质</h2><p>bias&amp;variance:h_hat是错误的假设函数，h是真实的假设函数（不是具体的sample，可以理解为理想的模型oracle）</p>
<ul>
<li>bias：针对单个模型的偏差，错误假设bias大，描述对本训练集的拟合程度</li>
<li>variance：针对多个模型，描述 不同数据集的模型输出差异，刻画数据扰动对模型的影响。过拟合时var大，注意与h(x)无关，所以从0开始</li>
</ul>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010131845.png" alt="image-20241101012602167"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010131973.png" alt="image-20241101013123907"></p>
<p>complexity</p>
<p>capacity：和特征&#x2F;参数数量相关</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010122339.png" alt="image-20241101012204270"></p>
<h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><ol>
<li>hold-out：随机分训练集和验证集</li>
<li>k折交叉验证</li>
<li>leave-one-out</li>
</ol>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>减小泛化误差</p>
<p>添加到loss function</p>
<h3 id="l2-ridge回归"><a href="#l2-ridge回归" class="headerlink" title="l2&#x2F;ridge回归"></a>l2&#x2F;ridge回归</h3><p>老师讲的l2一直是不开根号的</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010152378.png" alt="image-20241101015254323"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202410121325983.png" alt="img"></p>
<h3 id="l1-lasso回归"><a href="#l1-lasso回归" class="headerlink" title="l1&#x2F;lasso回归"></a>l1&#x2F;lasso回归</h3><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010154035.png" alt="image-20241101015413976"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010204671.png" alt="image-20241101020448640"></p>
<h2 id="贝叶斯统计-MAP"><a href="#贝叶斯统计-MAP" class="headerlink" title="贝叶斯统计&#x2F;MAP"></a>贝叶斯统计&#x2F;MAP</h2><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010816251.png" alt="image-20241101081604189"></p>
<p>下面是加入l2正则项</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010822278.png" alt="image-20241101082206212"></p>
<p>与MLE相比，考虑了先验分布</p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411010816654.png" alt="image-20241101081655600"></h1><h1 id="期中复盘"><a href="#期中复盘" class="headerlink" title="期中复盘"></a>期中复盘</h1><ol>
<li>mercer的定义，如何证明两个kernel之和仍然为kernel（没认真看，哭死）</li>
<li>证明LDA是线性函数，作业WA2有过程。我不知道直接写p&#x3D;0.5了行不行（感觉不太行xs）</li>
<li>θmap和θmle在θ为Uniform（均匀）时相等（没复习到）</li>
<li>反向传播考了max，画图没写箭头<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411011419460.png" alt="959633fda875456e403c4c6327c5b74"></li>
<li>以后如果带纸的考试一定要用ppt截图，不要手抄公式很慢容易出错，此外注意重点</li>
</ol>
<h1 id="Lec7"><a href="#Lec7" class="headerlink" title="Lec7"></a>Lec7</h1><p>capacity：拟合不同种类函数的能力</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081006496.png" alt="image-20241108100554275"></p>
<p>用y&#x3D;h(x)+ε来formalize</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081008634.png" alt="image-20241108100842505"></p>
<p>bias反映training data，variance反映testing data</p>
<h2 id="经验风险误差ERM"><a href="#经验风险误差ERM" class="headerlink" title="经验风险误差ERM"></a>经验风险误差ERM</h2><p>在理论研究前做一些约束：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081019483.png" alt="image-20241108101909407">只知道训练数据和测试数据都从分布D来，但不知道分布D具体是什么，后面通过计算找差异</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081030024.png" alt="image-20241108103047888"></p>
<h3 id="有限"><a href="#有限" class="headerlink" title="有限"></a>有限</h3><p>case1：假设有限的情况：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081030603.png" alt="image-20241108103032456"></p>
<p>两个著名的不等式：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081033578.png" alt="image-20241108103300517"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081045646.png" alt="image-20241108104543590"></p>
<p>左边&#x3D;1，&lt;&#x3D;2ke^{}，所以&gt;&#x3D;1-2ke^{}</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081054880.png" alt="image-20241108105424813"></p>
<h3 id="无限"><a href="#无限" class="headerlink" title="无限"></a>无限</h3><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081127284.png" alt="image-20241108112701196"></p>
<p>为了学好（即达到上述的误差保证），所需的样本数量必须与参数的维度 d<em>d</em> 成线性关系。这意味着，随着参数数量的增加，我们需要的样本数量也会线性增加。</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081147146.png" alt="image-20241108114708090"></p>
<p>尽管两个假设的参数数量不同，但它们实际上是等价的。</p>
<p>所以需要一个与参数化选择无关的复杂度度量，以便更准确地评估假设空间的复杂度</p>
<h4 id="VC-Dimension"><a href="#VC-Dimension" class="headerlink" title="VC Dimension"></a>VC Dimension</h4><p>用来衡量一个模型的复杂度，定义为：在该模型对应的空间中随机撒x点，然后对其中的每个点随机分配一个2类标签，使用你的模型来分类，并且要分对，请问x至多是多少。这个x就是VC维。</p>
<p>1.线性函数</p>
<p>如果选用二维空间中的直线作为判别函数，该分类模型的VC维是多少？<br>答案：3<br>解释：如果是3个点，无论如何随机的打标签，都存在一条直线都可以将两类样本分开。<br>如果是4个点，就可能会出现一种标签序列，使得不存在一条直线将两类样本分开。如下图。<br><img src="https://i-blog.csdnimg.cn/blog_migrate/263b69a231817e06f211089af7232e4d.png" alt="在这里插入图片描述"></p>
<p>如果选用三维空间中的直线作为判别函数，该分类模型的VC维是多少？<br>答案：4<br>总结：<br><img src="https://i-blog.csdnimg.cn/blog_migrate/8f0561c46678a4092f779b00bae5a0e0.png" alt="在这里插入图片描述"></p>
<p>用VC维度，即可知道无限H的上界</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081155182.png" alt="image-20241108115549092"></p>
<p>DNN中对于size相同（权重的数量相同），层数越多，VC维越大</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081200861.png" alt="image-20241108120039783"></p>
<p>总结内容：</p>
<ol>
<li><strong>控制泛化能力</strong>：我们可以通过调整假设 H 的复杂度来控制模型的泛化能力。</li>
<li><strong>VC维度作为复杂度的度量</strong>：VC维度是一个有用的复杂度度量工具。</li>
<li><strong>学习算法性能的界限</strong>：我们可以根据VC维度 VC(H) 和我们拥有的数据量来界定学习算法的性能。</li>
</ol>
<p>VC维度的局限性：</p>
<ol>
<li><strong>界限不够紧密</strong>：由于VC维度与数据分布无关，所以它提供的界限可能不够紧密。</li>
<li><strong>仅适用于二元分类</strong>：VC维度的定义仅适用于<strong>二元分类问题</strong>。</li>
</ol>
<p><strong>作业</strong></p>
<p>将每个点分配2类标签，看有多少种可能性，概念类能否把每种可能性都表示出来</p>
<h2 id="Rademacher-Complexity"><a href="#Rademacher-Complexity" class="headerlink" title="Rademacher  Complexity"></a>Rademacher  Complexity</h2><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411081204370.png" alt="image-20241108120412275"></p>
<p>衡量一个函数类（假设类）对随机噪声拟合能力的度量。如果一个假设类F能很好地拟合随机噪声（比如随机取值为 +1或 -1 的数据），那么它的 Rademacher Complexity 较高。</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411231649307.png" alt="image-20241123164937204"></p>
<p>S是抽取的固定样本，样本S是从分布D中独立抽取的m个样本</p>
<p>R_s是单个样本集S的复杂度度量，R_m是所有可能的样本集S进行期望的结果</p>
<p>sup是选最小上界，大部分时候可以理解为最大值</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411231733934.png" alt="image-20241123173310870"></p>
<p>sup操作关注的是最大幅度，与符号无关。</p>
<p>它描述的是函数类 F 的拟合能力，数值大小反映了函数类的复杂性，越复杂的函数类，Rademacher Complexity 越大，但永远不会小于 0。所以有a的绝对值。</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411231835641.png"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411232028617.png" alt="image-20241123202822556"></p>
<p>在R(H)中y_i因为取正负1，所以可以多乘以y。（感觉在这里正符号不用管）</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411232032216.png" alt="image-20241123203227160"></p>
<h1 id="Lec8-无监督学习"><a href="#Lec8-无监督学习" class="headerlink" title="Lec8 无监督学习"></a>Lec8 无监督学习</h1><p>良好表示的特点：低维、稀疏表示（更好的可解释性）、独立表示</p>
<h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411232141048.png" alt="image-20241123214122995"></p>
<h2 id="Spectral-Clustering"><a href="#Spectral-Clustering" class="headerlink" title="Spectral Clustering"></a>Spectral Clustering</h2><blockquote>
<p>我的理解是用图论知识做聚类</p>
</blockquote>
<p>degree：看wij的和</p>
<p>ratleigh-ritz定理：用拉格朗日乘子法</p>
<h3 id="RatioCut和NCut"><a href="#RatioCut和NCut" class="headerlink" title="RatioCut和NCut"></a>RatioCut和NCut</h3><p>为了避免最小切图导致的切图效果不佳，我们需要对每个子图的规模做出限定，一般来说，有两种切图方式，第一种是RatioCut，第二种是Ncut。</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411241521132.png" alt="image-20241124152059032"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411241528644.png" alt="image-20241124152842584"></p>
<p>选cut值小的，意味着这种划分在考虑节点度的情况下，<strong>两个子集之间的连接更稀疏</strong>。</p>
<ul>
<li>图中提到这些切割问题都是 NP - hard 的，即很难在多项式时间内找到最优解。</li>
<li>谱聚类（Spectral clustering）是解决这些问题的一种松弛方法，它通过将问题转化为特征向量和特征值的计算来近似求解。（比割和归一化割都可以用光谱法进行近似。）</li>
</ul>
<h3 id="Unnormalized-Spectral-Clustering算法"><a href="#Unnormalized-Spectral-Clustering算法" class="headerlink" title="Unnormalized Spectral Clustering算法"></a>Unnormalized Spectral Clustering算法</h3><p>算法输入：样本相似矩阵S和要聚类的类别数K。</p>
<ul>
<li>根据矩阵S建立权重矩阵W、三角矩阵D；</li>
<li>建立Laplacian矩阵L；</li>
<li>求矩阵L的（除0外）前K小个特征值及其对应的特征向量；</li>
<li>以这K组特征向量组成新的矩阵，其行数为样本数，列数为K，这里就是做了降维操作，从N维降到K维；</li>
<li>使用k-means<strong>等其它聚类算法</strong>进行聚类，得到K个Cluster。</li>
</ul>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411241717310.png" alt="image-20241124171702269"></p>
<h3 id="Normalized-spectral-clustering"><a href="#Normalized-spectral-clustering" class="headerlink" title="Normalized spectral clustering"></a>Normalized spectral clustering</h3><p>算法输入：样本相似矩阵S和要聚类的类别数K。</p>
<ul>
<li>根据矩阵S建立权重矩阵W、三角矩阵D；</li>
<li>建立Laplacian矩阵L以及L’ &#x3D; D-1L ；<ul>
<li>注意ppt用的不是对称归一化拉普拉斯矩阵，而是随机游走归一化拉普拉斯矩阵<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411242015143.png" alt="image-20241124201546113"></li>
</ul>
</li>
<li>求矩阵L’的前K小个特征值及其对应的特征向量；<ul>
<li>注意这里最小的特征值为0，所以有人在下一步会选第二小的特征值对应的特征向量进行聚类</li>
</ul>
</li>
<li>利用q’ &#x3D; D1&#x2F;2q求得对应的K个q；（q不是L的特征向量）</li>
<li>以这K组特征向量组成新的矩阵，其行数为样本数N，列数为K；</li>
<li>使用k-means<strong>等其它聚类算法</strong>进行聚类，得到K个Cluster。</li>
</ul>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202411241717582.png" alt="image-20241124171714538"></p>
<p>这里Lv&#x3D;λDv其实就是D^{-1}Lv&#x3D;λv，即对归一化的L_rw求特征值</p>
<h1 id="Lec9-PCA"><a href="#Lec9-PCA" class="headerlink" title="Lec9 PCA"></a>Lec9 PCA</h1><p>动机：输入特性包含大量的冗余性</p>
<p>主成分分析：将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分（是原始数据的线性组合），是在原有n维特征的基础上重新构造出来的k维特征。这对于数据压缩和降维非常有用。</p>
<ul>
<li>只关注主成分向量方向，而不是大小，所以认为正交特征的l2范数为1</li>
</ul>
<p>步骤：</p>
<ol>
<li><strong>数据标准化</strong>：对数据进行标准化处理，使得每个特征的均值（Mean⁡(x)）为0，方差（Var⁡(x_j)）为1<ol>
<li>中心化：减去均值</li>
<li>缩放：除以标准差<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412082008695.png" alt="image-20241208200843578"></li>
</ol>
</li>
<li><strong>找到第一个主成分</strong>：在标准化后的数据上，找到第一个主成分 u1，它是原始数据 x 的一个投影，具有最大的方差。<ol>
<li>投影长度是x^(i)Tu（这是一个标量，表示x^(i)在u方向上的分量的大小），所以目标是找到一个单位向量u，使得 uTΣu最大化<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412082016713.png" alt="image-20241208201629651"></li>
<li>虽然可以用优化工具来解决，但有个有效的解决方案：<em>u</em>1 是协方差矩阵Σ 的最大特征向量，选择最大的特征值 λ1 对应的特征向量 v1<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412082019018.png" alt="image-20241208201957940"></li>
</ol>
</li>
<li><strong>迭代找到其他主成分</strong>：对于 j&#x3D;2,…,n，依次找到其他的主成分 u_j。每个新的主成分都是数据x的一个投影，且具有最大的方差，同时要求这个主成分与之前找到的所有主成分（u1,…,uj−1）<strong>正交</strong>。<ol>
<li>第 j个主成分 uj 是协方差矩阵 Σ的第 j 大的特征向量。通过对L(u)求导证明<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412082029004.png" alt="image-20241208202935924"></li>
</ol>
</li>
</ol>
<p>所以过程可以分为：标准化数据-&gt;计算协方差矩阵Σ-&gt;特征值分解找到主成分u1,…,un。协方差矩阵Σ可以分解为Σ&#x3D;UΛUT，其中U是一个正交矩阵，其列向量是协方差矩阵的特征向量，Λ是一个对角矩阵，其对角线上的元素是对应的特征值。</p>
<p>PCA的另一种解释：基于投影残差最小化。假设有m个样本，尝试使用这些投影或图像向量来表示原始数据（投影是最能代表原始数据的方向（主成分））。在真实数据和它们的表示（投影残差）之间会存在误差，我们自然希望最小化这些误差。</p>
<h1 id="Lec10-强化学习-深度Q算法"><a href="#Lec10-强化学习-深度Q算法" class="headerlink" title="Lec10 强化学习&#x2F;深度Q算法"></a>Lec10 强化学习&#x2F;深度Q算法</h1><p>在课堂上，我们学习了强化学习。在本次作业中，你需要设计一个深度 Q 学习网络来玩 <em>CartPole</em> 游戏。我们的实验包含三个部分：</p>
<p>如果你想在代码执行时看到动画，你可以：</p>
<ol>
<li><strong>重启内核</strong>：这可以清除任何可能的错误或缓存，确保从干净的状态开始。</li>
<li><strong>根据最后一个代码块中的注释修改代码</strong>：通常代码块会给出如何启用或调整动画显示的提示，按照这些建议修改代码。</li>
</ol>
<p>由于制作动画通常非常耗时，因此建议你在渲染动画之前，确保你的代码没有错误并且运行正确。这样可以节省时间，避免因为错误代码而浪费渲染资源。</p>
<p><a target="_blank" rel="noopener" href="https://www.gymlibrary.dev/environments/classic_control/cart_pole/">CartPole</a> 是一个经典的强化学习环境，来自 <a target="_blank" rel="noopener" href="https://gym.openai.com/">gym</a> 包，由 <a target="_blank" rel="noopener" href="https://openai.com/">OpenAI</a> 设计。在 CartPole 环境中，一个杆子通过一个无驱动的关节连接到一个小车上，小车可以沿着无摩擦的轨道移动。如果小车不移动，杆子会因为重力而倒下。</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412052246786.png" alt="image-20241205224352765"></p>
<p>在这个环境中，目标是控制小车的运动，以保持杆子竖直不倒。强化学习算法（如 Q 学习）通常用于解决这一类平衡控制问题。在 CartPole 中，智能体需要通过不断调整小车的方向，来平衡杆子，避免其倒下。</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412052246796.png" alt="image-20241205224613210"></p>
<p>深度Q算法：让期望回报最大</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412061136305.png" alt="image-20241206113632181"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412061136986.png" alt="image-20241206113650874"></p>
<ul>
<li>回放记忆D：用于存储智能体与环境交互时的经验（即状态、动作、奖励、下一个状态），容量设为N，用于从中随机采样训练数据。</li>
<li>Q值模型：使用一个神经网络（参数为θ）来近似表示Q-值函数。即预测未来的累计奖励。</li>
<li>训练过程：<ul>
<li>回合：每次智能体与环境交互，经历一个完整的学习过程。每个回合从环境中获取一个初始状态s1开始。</li>
<li>时间步：每个回合中，智能体按照以下步骤进行决策和学习，直到回合结束<ul>
<li><strong>选择动作</strong>：基于当前的状态s_t，智能体以概率ϵ选择一个随机动作a_t（探索）；否则，选择一个使得Qθ(st,at)最大的动作a_t（利用当前的Q值，选择最优动作）。这个策略是ϵ-贪婪策略，<strong>平衡了探索和利用</strong>。<ul>
<li>局部最优不一定是全局最优，所以需要随机</li>
</ul>
</li>
<li><strong>执行动作并获得反馈</strong>：执行选择的动作at，环境返回奖励rt和下一个状态st+1。这些数据存储在回放记忆D中。</li>
<li><strong>存储转移</strong>：将当前的转移信息（即当前状态、所选动作、奖励、下一个状态）存储到回放记忆中。</li>
<li><strong>从回放记忆中采样</strong>：从回放记忆D中随机采样一个小批量的转移数据Ω。这有助于打破数据间的相关性，提升训练稳定性。<ul>
<li>称为“经验回放”。</li>
</ul>
</li>
<li><strong>计算目标值 yj</strong>：目标值就是最大化后的奖励。使用上面采样的转移数据，对模型进行更新。<ul>
<li>输入：经验回放记忆D，网络模型Q，批量大小batch_size，输出：更新后的网络模型Q</li>
<li>SGD：0.0005；BGD：0.001<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412061402119.png" alt="image-20241206140203058"></li>
</ul>
</li>
<li><strong>更新 Q 值模型</strong>：我们知道奖励机制，所以根据实际的奖励调整Q模型。MSE loss（平方差的和&#x2F;数量）<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412061407586.png" alt="image-20241206140728546"></li>
</ul>
</li>
</ul>
</li>
<li><strong>迭代训练</strong>：这个过程会在多个回合和时间步中重复进行。随着训练的进行，Q-值模型会不断地调整，智能体能够根据环境的反馈逐渐学习到最优策略，从而在给定状态下选择最佳动作。</li>
</ul>
<p>我们将使用一个 <code>deque</code>（双端队列）对象作为记忆 来存储转移过程。在这里，我们打算复用在编程作业 3（PA3）中构建的简单神经网络 <code>SimpleNN</code>，为深度 学习构建一个 <strong>模型</strong>。现在提供给你一个名为 <code>nn.py</code> 的 Python 文件，所以你无需从编程作业 3 中复制代码了。现在，让我们导入 <code>nn</code> 和 <code>deque</code>，以便能够使用它们。</p>
<p>由于只有两个可能的动作（0或1，表示小车向左或者向右），我们可以直接预测在某个状态下执行这两个动作的Q值，而不需要将动作作为输入特征的一部分：</p>
<p>为了实现这一点，我们只需将输出维度设置为2。Q模型以状态作为输入，因此输入维度设置为4。其他部分可以保持与PA3相同，因此我们的Q模型包含以下层：</p>
<p>L0，输入层（形状：$N \times 4$），其中$N$是批量大小。</p>
<ul>
<li>这里的连续状态空间（位置、速度、角度、角速度）(x,θ,x.,θ.)</li>
</ul>
<p>L1，线性层（形状：$4 \times 80$）</p>
<ul>
<li>输入维度是4，输出维度是80。说明W是4×80的矩阵，b是一个80维的偏置向量。</li>
</ul>
<p>L2，ReLU层</p>
<p>L3，线性层（形状：$80 \times 80$）</p>
<p>L4，ReLU层</p>
<p>L5，输出层（线性层，形状：$80 \times 2$）</p>
<ul>
<li>动作空间只有两个可能的动作：向左推（0）和向右推（1）</li>
<li>输出2个Q值，选择奖励大的动作就行</li>
</ul>
<p>然后跟随机策略进行对比，发现Q算法让平衡时间增长（time回合数增加），因为停止就不平衡了，让不停止时reward增加</p>
<p>Q3：在这个实验中，当智能体选择动作时我们采用了动态探索率epsilon。请解释代码中是如何控制探索率的，以及为什么这种方式是有效的？</p>
<p>在每个episode结束后，如果epsilon大于最小值，就乘以epsilon_decay进行衰减，这种方式实现了探索和利用的平衡，帮助智能体从广泛探索状态空间逐渐转向利用已学到的经验，避免陷入局部最优解。</p>
<h1 id="Lec11-ICA"><a href="#Lec11-ICA" class="headerlink" title="Lec11 ICA"></a>Lec11 ICA</h1><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><ol>
<li><p>鸡尾酒会问题：图中展示了一个场景，在一个房间里有多个声音源（Sources），并且在房间的不同位置放置了多个麦克风（Microphones）。每个麦克风都会记录下多个声音源混合在一起的声音（Mixtures）。问题的关键在于如何从这些混合的声音中分离出各个独立的声音源（Separated Sources）。<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412092037060.png" alt="image-20241209203707898"></p>
</li>
<li><p>脑电分析：常见的去除伪迹方法包括：ICA、滤波、手动校正<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412092050819.png" alt="image-20241209205053705"></p>
</li>
<li><p>脑部成像<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412092053962.png" alt="image-20241209205355849"></p>
</li>
</ol>
<h2 id="ICA"><a href="#ICA" class="headerlink" title="ICA"></a>ICA</h2><p>建模成盲源分离问题：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412092107159.png" alt="image-20241209210703094"></p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412092127036.png" alt="image-20241209212739985"></p>
<p>ICA的模糊性：</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412092131275.png" alt="image-20241209213116197"></p>
<p>一个关于密度和线性变换的定理（Theorem 1）（概率密度函数给出了随机变量在某个区间内取值的概率）<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412092202206.png" alt="image-20241209220225165"></p>
<p>算法：迭代优化：初始化W，假设Ps，然后迭代更新W直到收敛，估计出s？这里|W|代表行列式</p>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412092209755.png" alt="image-20241209220925678"></p>
<p>目标是通过最大化对数似然函数来估计解混矩阵W，从而实现对源信号的分离<img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412092230608.png" alt="image-20241209223026548"></p>
<p>ICA等价于学习投影方向(w_1, …, w_n)，在这些投影方向上满足以下条件：  </p>
<ul>
<li><strong>最大化投影信号的非高斯性总和</strong>    - 非高斯性是ICA的一个关键特性。由于高斯分布的旋转不变性，ICA通过寻找非高斯性来分离信号。  </li>
<li><strong>最小化投影信号的互信息</strong>    - 互信息（mutual information）是衡量两个随机变量之间相关性的指标。在ICA中，通过最小化投影信号之间的互信息来确保它们尽可能独立。 </li>
<li><strong>约束条件</strong>    - 约束条件是投影后的信号(w_1^T x, …, w_n^T x)是不相关的。这意味着在这些投影方向上，信号之间没有线性相关性。</li>
</ul>
<p><img src="https://lapsey-pictures.oss-cn-shenzhen.aliyuncs.com/typora_imgs/202412092241711.png" alt="image-20241209224131592"></p>

        </div>

        
            <section class="post-copyright">
                
                
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>标签:</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E8%AF%BE%E7%A8%8B/"># 课程</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">返回</a>
                <span>· </span>
                <a href="/">主页</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2024/11/07/%E5%AD%97%E8%8A%82%E9%9D%92%E8%AE%AD%E8%90%A5AI%E6%96%B9%E5%90%91%E7%AC%94%E8%AE%B0/">字节青训营AI方向笔记</a>
            
            
            <a class="next" rel="next" href="/2024/11/01/2024%E7%A7%8B%E5%AD%A3%E3%80%8A%E4%BF%A1%E6%81%AF%E8%AE%BA%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">2024秋季《信息论》课程笔记</a>
            
        </section>


    </article>
</div>


    <div id="gitalk-container"></div>
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: '',
        clientSecret: '',
        repo: 'blog_comment',
        owner: 'lyxx2535',
        admin: 'lyxx2535',
        id: md5(location.pathname),
        labels: 'Gitalk'.split(',').filter(l => l),
        perPage: 10,
        pagerDirection: 'last',
        createIssueManually: true,
        distractionFreeMode: false
      })
      gitalk.render('gitalk-container')
</script>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Annie | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a> | 2020 - 2025
            <br>
            <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<span class="site-uv">
    Total visitors:
    <i class="busuanzi-value" id="busuanzi_value_site_uv"></i>
</span>&nbsp;|&nbsp;


<span class="site-pv">
    Total views:
    <i class="busuanzi-value" id="busuanzi_value_site_pv"></i>
</span>

          </span>
    </div>
</footer>

    </div>
</body>

</html>